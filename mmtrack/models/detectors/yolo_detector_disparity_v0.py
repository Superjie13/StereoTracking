import torch
from mmdet.models.detectors.single_stage import SingleStageDetector
from mmdet.utils import ConfigType, OptConfigType, OptMultiConfig
from mmengine.dist import get_world_size
from mmengine.logging import print_log, MMLogger

from mmtrack.registry import MODELS


@MODELS.register_module()
class YOLODetector_Disparity_V0(SingleStageDetector):
    r"""Implementation of YOLO Series

    Args:
        backbone (:obj:`ConfigDict` or dict): The backbone config.
        neck (:obj:`ConfigDict` or dict): The neck config.
        bbox_head (:obj:`ConfigDict` or dict): The bbox head config.
        train_cfg (:obj:`ConfigDict` or dict, optional): The training config
            of YOLOX. Defaults to None.
        test_cfg (:obj:`ConfigDict` or dict, optional): The testing config
            of YOLOX. Defaults to None.
        data_preprocessor (:obj:`ConfigDict` or dict, optional): Config of
            :class:`DetDataPreprocessor` to process the input data.
            Defaults to None.
        init_cfg (:obj:`ConfigDict` or list[:obj:`ConfigDict`] or dict or
            list[dict], optional): Initialization config dict.
            Defaults to None.
        use_syncbn (bool): whether to use SyncBatchNorm. Defaults to True.
    """

    def __init__(self,
                 backbone: ConfigType,
                 neck: ConfigType,
                 bbox_head: ConfigType,
                 train_cfg: OptConfigType = None,
                 test_cfg: OptConfigType = None,
                 data_preprocessor: OptConfigType = None,
                 init_cfg: OptMultiConfig = None,
                 use_syncbn: bool = True):
        super().__init__(
            backbone=backbone,
            neck=neck,
            bbox_head=bbox_head,
            train_cfg=train_cfg,
            test_cfg=test_cfg,
            data_preprocessor=data_preprocessor,
            init_cfg=init_cfg)

        # TODOï¼š Waiting for mmengine support
        if use_syncbn and get_world_size() > 1:
            torch.nn.SyncBatchNorm.convert_sync_batchnorm(self)
            print_log('Using SyncBatchNorm()', 'current')

    def init_weights(self):
        if self.init_cfg is not None:
            if self.init_cfg.get('type', None) == 'ColorPretrained':
                from mmengine.runner.checkpoint import _load_checkpoint, load_state_dict
                logger = MMLogger.get_instance('mmengine')

                pretrained = self.init_cfg.get('checkpoint')
                print_log(f"load model from: {pretrained}", logger=logger)
                checkpoint = _load_checkpoint(pretrained, logger=logger, map_location='cpu')
                state_dict = checkpoint['state_dict']

                print_log(f"update stem conv for disparity: `concatenate`", logger=logger)

                stem_conv_dict = state_dict['backbone.stem.conv.conv.weight']
                state_dict['backbone.stem.conv.conv.weight'] = torch.cat([stem_conv_dict, stem_conv_dict], dim=1)
                load_state_dict(self, state_dict, strict=False, logger=logger)